{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "from nltk import ngrams\n",
    "from spacy_wordnet.wordnet_annotator import WordnetAnnotator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Sent_1  POS_1          Sensekey_1       Sent_2  POS_2  \\\n",
      "0         do    AUX      make%2:41:00::          how    ADV   \n",
      "1        you   PRON                   -           it   PRON   \n",
      "2      train   VERB     train%2:31:01::           is    AUX   \n",
      "3        for    ADP                   -           we   PRON   \n",
      "4    passing   VERB      pass%2:38:00::         have    AUX   \n",
      "5      tests   NOUN     trial%1:09:00::           so    ADV   \n",
      "6         or  CCONJ    oregon%1:15:00::         much    ADJ   \n",
      "7         do    AUX      make%2:41:00::  information   NOUN   \n",
      "8        you   PRON                   -            ,  PUNCT   \n",
      "9      train   VERB     train%2:31:01::          but  CCONJ   \n",
      "10       for    ADP                   -         know   VERB   \n",
      "11  creative    ADJ  creative%3:00:00::           so    ADV   \n",
      "12   inquiry   NOUN   inquiry%1:09:01::       little    ADJ   \n",
      "\n",
      "                          Sensekey_2          Sent_3  POS_3  \\\n",
      "0                                  -              it   PRON   \n",
      "1   information_technology%1:09:00::              is    AUX   \n",
      "2                       be%2:42:03::             the    DET   \n",
      "3                                  -  responsibility   NOUN   \n",
      "4                     have%2:40:00::              of    ADP   \n",
      "5                       so%4:02:02::   intellectuals   NOUN   \n",
      "6                     much%3:00:00::              to   PART   \n",
      "7              information%1:10:00::           speak   VERB   \n",
      "8                                  -             the    DET   \n",
      "9                   merely%4:02:00::           truth   NOUN   \n",
      "10                    know%2:31:01::             and  CCONJ   \n",
      "11                      so%4:02:02::          expose   VERB   \n",
      "12                   small%3:00:00::            lies   NOUN   \n",
      "\n",
      "                          Sensekey_3  \n",
      "0   information_technology%1:09:00::  \n",
      "1                       be%2:42:03::  \n",
      "2                                  -  \n",
      "3                     duty%1:04:00::  \n",
      "4                                  -  \n",
      "5             intellectual%1:18:00::  \n",
      "6                                  -  \n",
      "7                     talk%2:32:00::  \n",
      "8                                  -  \n",
      "9                    truth%1:09:00::  \n",
      "10                                 -  \n",
      "11                  expose%2:39:02::  \n",
      "12                     lie%1:10:00::  \n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(WordnetAnnotator(nlp.lang), after='tagger')\n",
    "columns = ['Sent_1','POS_1', 'Sensekey_1','Sent_2','POS_2','Sensekey_2', 'Sent_3','POS_3','Sensekey_3']\n",
    "df_ = pd.DataFrame(columns=columns)\n",
    "sent1 = \"do you train for passing tests or do you train for creative inquiry\"\n",
    "encent = nlp(sent1)\n",
    "df_['POS_1'] = [token.pos_ for token in encent]\n",
    "df_['Sent_1']= [token.text for token in encent]\n",
    "df_['Sensekey_1']= [token._.wordnet.lemmas()[0].key() if len(token._.wordnet.lemmas()) is not 0 else '-' for token in encent]\n",
    "\n",
    "sent2 = \"how it is we have so much information, but know so little\"\n",
    "encent = nlp(sent2)\n",
    "df_['POS_2'] = [token.pos_ for token in encent]\n",
    "df_['Sent_2']= [token.text for token in encent]\n",
    "df_['Sensekey_2']= [token._.wordnet.lemmas()[0].key() if len(token._.wordnet.lemmas()) is not 0 else '-' for token in encent]\n",
    "\n",
    "sent3 = \"it is the responsibility of intellectuals to speak the truth and expose lies\"\n",
    "encent = nlp(sent3)\n",
    "df_['POS_3'] = [token.pos_ for token in encent]\n",
    "df_['Sent_3']= [token.text for token in encent]\n",
    "df_['Sensekey_3']= [token._.wordnet.lemmas()[0].key() if len(token._.wordnet.lemmas()) is not 0 else '-' for token in encent]\n",
    "\n",
    "# for token in encent:\n",
    "#     print(token.text, token.pos_)\n",
    "#     if len(token._.wordnet.synsets()) is not 0: print(token._.wordnet.synsets()[0])\n",
    "#     if len(token._.wordnet.lemmas()) is not 0: print(token._.wordnet.lemmas()[0].key())\n",
    "#     break\n",
    "print(df_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         level  outside  devices  shoa  fine  disappointment(it  horror  \\\n",
      "level       24        0        0     0     0                  0       0   \n",
      "outside      0        0        0     0     0                  0       0   \n",
      "devices      0        0        0     0     0                  0       0   \n",
      "shoa         0        0        0     0     0                  0       0   \n",
      "fine         0        0        0     0     0                  0       0   \n",
      "...        ...      ...      ...   ...   ...                ...     ...   \n",
      "raise        0        0        0     0     0                  0       0   \n",
      "amazing      2        0        0     0     0                  0       0   \n",
      "recover      0        0        0     0     0                  0       0   \n",
      "worth        0        0        0     0     0                  0       0   \n",
      "2-pack       0        0        0     0     0                  0       0   \n",
      "\n",
      "         facts  dime  forces  ...  peppy  3&quot  shiek  thriller  horde  \\\n",
      "level        0     0       0  ...      0       0      0         0      0   \n",
      "outside      0     0       0  ...      0       0      0         0      0   \n",
      "devices      0     0       0  ...      0       0      0         0      0   \n",
      "shoa         0     0       0  ...      0       0      0         0      0   \n",
      "fine         0     0       0  ...      0       0      0         0      0   \n",
      "...        ...   ...     ...  ...    ...     ...    ...       ...    ...   \n",
      "raise        0     0       0  ...      0       0      0         0      0   \n",
      "amazing      0     0       0  ...      0       0      0         0      0   \n",
      "recover      0     0       0  ...      0       0      0         0      0   \n",
      "worth        0     0       0  ...      0       0      0         0      0   \n",
      "2-pack       0     0       0  ...      0       0      0         0      0   \n",
      "\n",
      "         raise  amazing  recover  worth  2-pack  \n",
      "level        0        2        0      0       0  \n",
      "outside      0        0        0      0       0  \n",
      "devices      0        0        0      0       0  \n",
      "shoa         0        0        0      0       0  \n",
      "fine         0        0        0      0       0  \n",
      "...        ...      ...      ...    ...     ...  \n",
      "raise        0        0        0      0       0  \n",
      "amazing      0        8        0      0       0  \n",
      "recover      0        0        0      0       0  \n",
      "worth        0        0        0      2       0  \n",
      "2-pack       0        0        0      0       0  \n",
      "\n",
      "[11716 rows x 11716 columns]\n"
     ]
    }
   ],
   "source": [
    "with open('1000-reviews.txt', encoding=\"utf8\") as g:\n",
    "    file_ = g.read().split()\n",
    "    vocab = list(set(file_))\n",
    "    df_ = pd.DataFrame(0 ,index=vocab, columns=vocab)\n",
    "    #print(len(vocab))\n",
    "    grams11 = ngrams(file_, 11)\n",
    "    for gram in grams11:\n",
    "        gram = list(gram)\n",
    "        mid_ele = gram[5]\n",
    "        gram.remove(mid_ele)\n",
    "        for in_g in gram:\n",
    "            df_[in_g][mid_ele] += 1\n",
    "    #print(len(grams11))\n",
    "    print(df_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "1\n",
      "1\n",
      "16\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(11716, 11716)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_['zelda']['does'])\n",
    "print(df_['fighting']['does'])\n",
    "print(df_['nintendo']['does'])\n",
    "print(df_['is']['does'])\n",
    "print(df_['red']['does'])\n",
    "df_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "pca = PCA(n_components=10, svd_solver='randomized',  random_state=40)\n",
    "pca.fit(df_)\n",
    "final = pca.transform(df_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11716"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([264.29158519, -55.13376576,  50.95735397,  40.90790006,\n",
       "         2.93419979, -22.02656516,  -7.74302021,   6.47667034,\n",
       "        -4.73014844, -12.83475311])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"word2vec.txt\", \"w\") as text_file:\n",
    "    text_file.write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
